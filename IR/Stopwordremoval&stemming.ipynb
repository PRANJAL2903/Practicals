{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R-W76PpyoCx",
        "outputId": "804334dd-74f0-4863-f2ae-35decef0f11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document=\"This is an feet of an example document for tokenziation. , This is an exampledocument for POS tagging, stemming.\""
      ],
      "metadata": {
        "id": "akH1dyKyyqLO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "token=word_tokenize(document)\n",
        "token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZb8LE_Py4i6",
        "outputId": "b1c32e37-7975-4d5e-84d7-770ca32f12cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'an',\n",
              " 'feet',\n",
              " 'of',\n",
              " 'an',\n",
              " 'example',\n",
              " 'document',\n",
              " 'for',\n",
              " 'tokenziation',\n",
              " '.',\n",
              " ',',\n",
              " 'This',\n",
              " 'is',\n",
              " 'an',\n",
              " 'exampledocument',\n",
              " 'for',\n",
              " 'POS',\n",
              " 'tagging',\n",
              " ',',\n",
              " 'stemming',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POS tagging"
      ],
      "metadata": {
        "id": "mw75K89X0fm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tagging=pos_tag(token)\n",
        "pos_tagging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQblmJxrzTL_",
        "outputId": "911fa1fe-2974-4ca2-8487-f1b327d14f2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('an', 'DT'),\n",
              " ('feet', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('example', 'NN'),\n",
              " ('document', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('tokenziation', 'NN'),\n",
              " ('.', '.'),\n",
              " (',', ','),\n",
              " ('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('an', 'DT'),\n",
              " ('exampledocument', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('POS', 'NNP'),\n",
              " ('tagging', 'NN'),\n",
              " (',', ','),\n",
              " ('stemming', 'VBG'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sem.logic import Tokens\n",
        "from nltk.corpus import stopwords\n",
        "stopword=set(stopwords.words('english'))\n",
        "filtered_tokens=[]\n",
        "for i in token:\n",
        "  if i.lower() not in stopword:\n",
        "    filtered_tokens.append(i)\n",
        "\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN6fkWpf0n7Z",
        "outputId": "22951cb7-e2df-44d6-b33d-bbf6ee834817"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feet',\n",
              " 'example',\n",
              " 'document',\n",
              " 'tokenziation',\n",
              " '.',\n",
              " ',',\n",
              " 'exampledocument',\n",
              " 'POS',\n",
              " 'tagging',\n",
              " ',',\n",
              " 'stemming',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming & Lemmatization"
      ],
      "metadata": {
        "id": "6PUVNy-r2PUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "stemm=PorterStemmer()\n",
        "stemmed_tokens=[]\n",
        "for i in token:\n",
        "  if i.lower() not in stopword:\n",
        "    stemmed_tokens.append(stemm.stem(i))\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yS6BsTR1M0k",
        "outputId": "6a152b78-246d-4297-dfa8-dfa2b5a397bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['feet', 'exampl', 'document', 'tokenzi', '.', ',', 'exampledocu', 'po', 'tag', ',', 'stem', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r488M2851vmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVUxn-2e1tVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}